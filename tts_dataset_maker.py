# -*- coding: utf-8 -*-
"""tts-dataset-maker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jY1-hClniFklo_O4s4KNuYkhZC5FZvQR
"""

!pip install yt-dlp assemblyai -q

!python tts_dataset_maker.py "https://www.youtube.com/watch?v=5CLwBITiqWA" --assemblyai-key "cb9cc78c60884cf78b9573aa37665a1a"

!pip install gradio

# If you're in Colab, install/upgrade deps (safe to re-run)
!pip -q install gradio==5.* soundfile pandas numpy

import json
import os
import io
import numpy as np
import soundfile as sf
import pandas as pd
import gradio as gr

# ---- Paths (adjust if yours differ) ----
JSON_PATH = "output/tts_dataset.json"
AUDIO_PATH = "output/audio.wav"

assert os.path.exists(JSON_PATH), f"Missing JSON at {JSON_PATH}"
assert os.path.exists(AUDIO_PATH), f"Missing audio at {AUDIO_PATH}"

# ---- Load data ----
with open(JSON_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

segments = data.get("segments", [])
if not segments:
    raise ValueError("No segments found in JSON under key 'segments'.")

# Normalize into a DataFrame
df = pd.DataFrame(segments)[["start", "end", "speaker", "text"]].copy()
df["start"] = df["start"].astype(float)
df["end"] = df["end"].astype(float)
df["duration_s"] = (df["end"] - df["start"]).round(3)
df.insert(0, "id", range(len(df)))  # stable row id

# ---- Load full audio once ----
full_audio, sr = sf.read(AUDIO_PATH, always_2d=False)  # shape: (n,) or (n, ch)
is_stereo = full_audio.ndim == 2
n_samples = full_audio.shape[0] if not is_stereo else full_audio.shape[0]
duration_total = n_samples / sr

def build_filtered_table(speaker_filter: str, query: str):
    """Return a filtered DataFrame for the UI."""
    dd = df.copy()
    if speaker_filter and speaker_filter != "All":
        dd = dd[dd["speaker"] == speaker_filter]
    if query:
        q = query.strip().lower()
        dd = dd[dd["text"].str.lower().str.contains(q, na=False)]
    # Nice ordering
    return dd[["id", "speaker", "start", "end", "duration_s", "text"]].reset_index(drop=True)

def on_filter_change(speaker_filter, query):
    table = build_filtered_table(speaker_filter, query)
    return table

def on_row_select(evt: gr.SelectData, current_table):
    """Row click handler from the DataFrame."""
    try:
        # Get the row index from the event
        row_idx = evt.index[0] if isinstance(evt.index, (list, tuple)) else evt.index
        if row_idx is None or row_idx < 0:
            return None, "", ""

        # Get the segment from the current filtered table
        if row_idx >= len(current_table):
            return None, "", ""

        seg = current_table.iloc[row_idx]
        seg_id = int(seg["id"])

        # Get the original row from the full dataframe
        row = df[df["id"] == seg_id]
        if row.empty:
            return None, "", ""

        row = row.iloc[0]
        start_s, end_s, speaker, text = row["start"], row["end"], row["speaker"], row["text"]

        # Create audio slice
        start_idx = int(round(start_s * sr))
        end_idx = int(round(end_s * sr))
        audio_slice = full_audio[start_idx:end_idx]

        return (sr, audio_slice), f"{speaker} — {start_s:.2f}s → {end_s:.2f}s", text

    except Exception as e:
        print(f"Error in row select: {e}")
        return None, "", ""

def concat_speaker(speaker_filter: str, query: str):
    """Concatenate all segments for the current filter (speaker + query)."""
    table = build_filtered_table(speaker_filter, query)
    if table.empty:
        return None, "No segments match.", ""

    # Build concatenation with 200 ms silence between segments
    silence_len = int(0.2 * sr)
    silence = np.zeros((silence_len, full_audio.shape[1])) if full_audio.ndim == 2 else np.zeros(silence_len)

    pieces = []
    transcript_parts = []
    for _, r in table.iterrows():
        start_s, end_s = float(r["start"]), float(r["end"])
        start_idx = int(round(start_s * sr))
        end_idx = int(round(end_s * sr))
        pieces.append(full_audio[start_idx:end_idx])
        transcript_parts.append(f"[{r['speaker']} {start_s:.2f}–{end_s:.2f}] {r['text']}")
        pieces.append(silence)

    if pieces:
        # Drop trailing silence
        pieces = pieces[:-1] if len(pieces) > 1 else pieces
        concat = np.concatenate(pieces, axis=0)
    else:
        return None, "No segments match.", ""

    # Return as (sr, np.array) for Gradio Audio
    meta = f"Concatenated {len(table)} segment(s)"
    full_text = "\n\n".join(transcript_parts)
    return (sr, concat), meta, full_text

with gr.Blocks(title="TTS Segment Explorer") as demo:
    gr.Markdown(
        f"# TTS Segment Explorer\n"
        f"- **Audio:** `{os.path.basename(AUDIO_PATH)}` ({duration_total:.2f}s @ {sr} Hz)\n"
        f"- **Segments:** {len(df)}\n\n"
        "Filter by speaker or search text, click a row to hear that exact slice. "
        "You can also play all matching segments concatenated."
    )

    with gr.Row():
        speaker_choices = ["All"] + sorted(df["speaker"].dropna().unique().tolist())
        speaker_dd = gr.Dropdown(speaker_choices, value="All", label="Speaker")
        search_tb = gr.Textbox(value="", label="Search text", placeholder="Type to filter by transcript…")

    table = gr.Dataframe(
        headers=["id", "speaker", "start", "end", "duration_s", "text"],
        value=build_filtered_table("All", ""),
        row_count=(len(df), "dynamic"),
        wrap=True,
        interactive=False,
        label="Segments (click a row to play)"
    )

    with gr.Row():
        seg_audio = gr.Audio(label="Selected Segment", interactive=False)
        seg_meta = gr.Markdown()
    seg_text = gr.Textbox(label="Segment Text", interactive=False, lines=5)

    with gr.Accordion("Play all matching segments (by current filter)", open=False):
        all_audio = gr.Audio(label="Concatenated Audio", interactive=False)
        all_meta = gr.Markdown()
        all_text = gr.Textbox(label="Combined Transcript", interactive=False, lines=10)
        play_all_btn = gr.Button("▶️ Concatenate & Play")

    # Events
    speaker_dd.change(on_filter_change, inputs=[speaker_dd, search_tb], outputs=table)
    search_tb.change(on_filter_change, inputs=[speaker_dd, search_tb], outputs=table)

    # Row selection -> play exact slice
    table.select(on_row_select, inputs=[table], outputs=[seg_audio, seg_meta, seg_text])

    # Button -> play all filtered
    play_all_btn.click(concat_speaker, inputs=[speaker_dd, search_tb], outputs=[all_audio, all_meta, all_text])

demo.launch()

